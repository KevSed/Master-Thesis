\chapter{Machine Learning and Random forests}
\label{ch:ML}
%
The analysis tasks described in \autoref{ch:iact} make strong use of machine
learning tools to be performed in the most efficient way. There are a lot of
different machine learning techniques that find increasingly many and
successfull applications in modern physics.

\section{Machine learning}
%
Machine learning describes the field of applied statistics that uses computers to learn to solve certain problems. The \textit{learning} is defined by~\cite{mitchell} as: \enquote{A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.} There is a wide variety of different tasks this can be applied to and many performance measures. The most important preposition for well suited problems is the available experience or in this case the amount of data to learn on.

Machine learning aims at solving problems that profit from the computing power
of modern technology but are not the kind of problem to be solved by a typical
program written by a human \cite{goodfellow}. The concept is to try to
reproduce the concept of intelligence within a computer.

\subsection{The Experience}
%
To make a generic machine learning algorithm work for a specific task it has to
be \textit{trained} on data. Just like a human, it needs to be given
information to base a decision on and to be shown how that decision based on
the information is supposed to look. The kind of data suitable for learning
depends on the kind of algorithm to be used and vice versa. Generally, machine
learning algorithms can be divided into \textbf{supervised} and
\textbf{unsupervised} algorithms, but in this work only supervised algorithms
will be used.

Data to derive experience from for supervised algorithms consists of data points
with a certain feature set and a specific label or true value. The datasets for
the \textit{separation} of two classes for example contain a certain feature
set for every data point and a label for the corresponding class each of these
points belongs to. Unsupervised learning therefore aims at learning to predict
a certain target value (or multiple) from a given feature set including this
value. The used learning data in this analysis is provided by simulations, so
that the true values for the machine learning tasks are known.

\subsection{The Task}
%
The task a machine learning algorithm is supposed to do is the solution of a
specific problem in the best possible way. To reach this solution the process
of learning is used but not the task. A machine learning algorithm used to
distinguish between to things is therefore given the task to distinguish two
things rather than to learn to distinguish these two. The three desired tasks
within the analysis of Cherenkov images are the \textit{separation} of
gamma-rays from hadronic cosmic rays, as well as the \textit{estimation} of the
energy and source position of cosmic gamma-rays. The kind of task already
determines what machine learning techniques are best to be used or which ones
don't suit the task and how working architectures have to look. A classification like the separation

\subsection{The Performance Measure}
%
As described above machine learning is about improving on certain tasks.
Therefore it is essential to quantify the performance during but also after the
learning process. This already implicates that performance measures are very
specific to the task. The performance of a classification task is naturally
measured by the \textbf{accuracy} of the model, because it simply describes how
many of the model's outputs are correct. When validating continuous outputs
rather than discrete classifications an accuracy is not appropriate. The
estimation of the energy, e.g., requires a continuous-valued metric.

The models are trained on specific, simulated data but only to be applied on
datasets they have \textit{not} been trained on. Thus, the interesting metrics
are those calculated on datasets complementary to the training data sets,
because they resemble the real use case. To do so, the whole dataset is divided
into a fraction determined for training and a test set on which the metrics can
be calculated. A frequently used method for this is the
\textbf{cross-validation}. The data set is divided into $n$ equaly sized, random
subsamples; $n-1$ samples are used for training whereas the single excluded
sample is used for validation. This is done $n$ times for each one of the
subsamples and the metrics calculated as the mean value of all the single
validations.

\section{Random forests}
%
One of the most frequently used machine learning algorithms and the one used
for the tasks in this analysis is the so called random forest. This is a
supervised learning algorithm based on the so called \textit{decision tree}.

\subsection{Decision Trees}
%
Decision trees classify data points based on consecutive binary decisions. The
decisions are based on the single features of the data point and result in a
point specific result that is being returned. The number of single binary
decisions (also called \textit{leaf}) preceding the final classification is
called \textit{depth} of the tree. Decision trees are trained on labelled data
and determine thresholds for every feature to classify the data point.
%
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance = 3cm, auto]
    \node (f1) [feature] {\texttt{feature\_1}};
    \node (f2) [feature, below of=f1, xshift=-2.4cm] {\texttt{feature\_2}};
    \node (f3) [feature, below of=f1, xshift=2.4cm] {\texttt{feature\_3}};

    \node (c1) [class1, below of=f2, xshift=-1.2cm] {\texttt{class 1}};
    \node (c2) [class2, below of=f2, xshift=1.2cm] {\texttt{class 2}};

    \node (c3) [class1, below of=f3, xshift=-1.2cm] {\texttt{class 1}};
    \node (c4) [class2, below of=f3, xshift=1.2cm] {\texttt{class 2}};

    \draw [pil] (f1) -- node[anchor=east] {$\mathbf{>15}$\;} (f2);
    \draw [pil] (f1) -- node[anchor=west] {\;$\mathbf{\leq15}$} (f3);
    \draw [pil] (f2) -- node[anchor=east] {$\mathbf{<100}$\;} (c1);
    \draw [pil] (f2) -- node[anchor=west] {\;$\mathbf{\geq100}$} (c2);
    \draw [pil] (f3) -- node[anchor=east] {$\symbf{<-\sfrac{\pi}{2}}$\;} (c3);
    \draw [pil] (f3) -- node[anchor=west] {\;$\symbf{\geq-\sfrac{\pi}{2}}$} (c4);
  \end{tikzpicture}
  \caption{Example sketch of a decision tree. The tree decides whether an input data point is of class 1 or class 2. The available features are \texttt{feature\_1}, \texttt{feature\_2} and \texttt{feature\_3}. This decision tree has a depth of 2 and solves the task of a binary separation at each leaf (green boxes). The decision thresholds are written next to the respective connecting lines.}
  \label{fig:tree}
\end{figure}
%
To determine the best decision threshold for each leaf, a performance measure
for the information gain is necessary. For a classification task the required
metric would be the accuracy. The thresholds are thus optimized to get the best
accuracy at the respective leaf. This way the decision tree is build from the
top leaf downwards until a perfect classification is achieved or until a set
maximum depth is reached.

\textbf{[REGRESSION]}

\subsection{Random forests}
%
A decision tree that is fitted to the data too extensively will reach
very high accuracies, but will suffer from a very bad generalization. At some
point the tree starts to adapt to the training data's specifics too much and
won't work on other data that does not have these specific characteristics.
This phenomenon is called \textit{overfitting} and generally is represented by
a large gap between training and test errors \cite{goodfellow}. A way to
prevent this from happening is to constrain the complexity of the decision tree
while using a large number of different trees. This way the trees are not
overfitted but the complete model is still complex enough.

Random forests are generated by a process called
\textit{bagging}~\cite{bagging}. Every tree within the forest is trained on a
subset of the data randomly sampled with replacement. This way, a large number
of slightly different trees is generated. While every single decision tree is
still prone to overfitting to noise in its respective dataset, averaging over
all of them is not, as long as the trees are not correlated. To prevent such
correlations and further generalize the model, additionally every decision node
inside a single tree is only given a random subset of all available features.
Random forests therefore have two parameters: the number of trees $n$ and the
number $k$ of available features at each node.

The output of the model consisting of such a forest is then generated
by counting trees with a specific decision. A confidence for a multi-class
separation, e.g., can be generated by counting the fraction of trees that
decided for the specific class. By setting a threshold on the confidence
classification decisions can be performed.

\subsection{Performance Measures}
%
As mentioned earlier, the right performance measure depends on the task to be
validated.

\textbf{Confusion matrix.} \blindtext

\textbf{Receiver Operating Characteristic.} \blindtext

\textbf{$\symbf{R^2}$ Score.} \blindtext

\chapter{Machine Learning and Random forests}
\label{ch:ML}
%
The analysis tasks described in \autoref{ch:iact} make strong use of machine
learning tools to be performed in the most efficient way. There are a lot of
different machine learning techniques that find increasingly many and
successfull applications in modern physics.

\section{Machine learning}
%
Machine learning describes the field of applied statistics that uses computers to learn to solve certain problems. The \textit{learning} is defined by~\cite{mitchell} as: \enquote{A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.} There is a wide variety of different tasks this can be applied to and many performance measures. The most important preposition for well suited problems is the available experience or in this case the amount of data to learn on.

Machine learning aims at solving problems that profit from the computing power
of modern technology but are not the kind of problem to be solved by a typical
program written by a human \cite{goodfellow}. The concept is to try to
reproduce the concept of intelligence within a computer.

\subsection{The Experience}
%
To make a generic machine learning algorithm work for a specific task it has to
be \textit{trained} on data. Just like a human, it needs to be given
information to base a decision on and to be shown how that decision based on
the information is supposed to look. The kind of data suitable for learning
depends on the kind of algorithm to be used and vice versa. Generally, machine
learning algorithms can be divided into \textbf{supervised} and
\textbf{unsupervised} algorithms, but in this work only supervised algorithms
will be used.

Data to derive experience from for supervised algorithms consists of data points
with a certain feature set and a specific label or true value. The datasets for
the \textit{separation} of two classes for example contain a certain feature
set for every data point and a label for the corresponding class each of these
points belongs to. Unsupervised learning therefore aims at learning to predict
a certain target value (or multiple) from a given feature set including this
value. The used learning data in this analysis is provided by simulations, so
that the true values for the machine learning tasks are known.

\subsection{The Task}
%
The task a machine learning algorithm is supposed to do is the solution of a
specific problem in the best possible way. To reach this solution the process
of learning is used but not the task. A machine learning algorithm used to
distinguish between to things is therefore given the task to distinguish two
things rather than to learn to distinguish these two. The three desired tasks
within the analysis of Cherenkov images are the \textit{separation} of
gamma-rays from hadronic cosmic rays, as well as the \textit{estimation} of the
energy and source position of cosmic gamma-rays. The kind of task already
determines what machine learning techniques are best to be used or which ones
don't suit the task and how working architectures have to look. A classification like the separation

\subsection{The Performance Measure}
%
As described above machine learning is about improving on certain tasks.
Therefore it is essential to quantify the performance during but also after the
learning process. This already implicates that performance measures are very
specific to the task. The performance of a classification task is naturally
measured by the \textbf{accuracy} of the model, because it simply describes how
many of the model's outputs are correct. When validating continuous outputs
rather than discrete classifications an accuracy is not appropriate. The
estimation of the energy, e.g., requires a continuous-valued metric.

The models are trained on specific, simulated data but only to be applied on
datasets they have \textit{not} been trained on. Thus, the interesting metrics
are those calculated on datasets complementary to the training data sets,
because they resemble the real use case. To do so, the whole dataset is divided
into a fraction determined for training and a test set on which the metrics can
be calculated. A frequently used method for this is the
\textbf{cross-validation}. The data set is divided into $n$ equaly sized, random
subsamples; $n-1$ samples are used for training whereas the single excluded
sample is used for validation. This is done $n$ times for each one of the
subsamples and the metrics calculated as the mean value of all the single
validations.

\section{Random forests}
%
One of the most frequently used machine learning algorithms and the one used
for the tasks in this analysis is the so called random forest. This is a
supervised learning algorithm based on the so called \textit{decision tree}.

\subsection{Decision Trees}
%
Decision trees classify data points based on consecutive binary decisions. The
decisions are based on the single features of the data point and result in a
point specific result that is being returned. The number of single binary
decisions (also called \textit{leaf}) preceding the final classification is
called \textit{depth} of the tree. Decision trees are trained on labelled data
and determine thresholds for every feature to classify the data point.
%
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance = 3cm, auto]
    \node (f1) [feature] {\texttt{feature\_1}};
    \node (f2) [feature, below of=f1, xshift=-2.4cm] {\texttt{feature\_2}};
    \node (f3) [feature, below of=f1, xshift=2.4cm] {\texttt{feature\_3}};

    \node (c1) [class1, below of=f2, xshift=-1.2cm] {\texttt{class 1}};
    \node (c2) [class2, below of=f2, xshift=1.2cm] {\texttt{class 2}};

    \node (c3) [class1, below of=f3, xshift=-1.2cm] {\texttt{class 1}};
    \node (c4) [class2, below of=f3, xshift=1.2cm] {\texttt{class 2}};

    \draw [pil] (f1) -- node[anchor=east] {$\mathbf{>15}$\;} (f2);
    \draw [pil] (f1) -- node[anchor=west] {\;$\mathbf{\leq15}$} (f3);
    \draw [pil] (f2) -- node[anchor=east] {$\mathbf{<100}$\;} (c1);
    \draw [pil] (f2) -- node[anchor=west] {\;$\mathbf{\geq100}$} (c2);
    \draw [pil] (f3) -- node[anchor=east] {$\symbf{<-\sfrac{\pi}{2}}$\;} (c3);
    \draw [pil] (f3) -- node[anchor=west] {\;$\symbf{\geq-\sfrac{\pi}{2}}$} (c4);
  \end{tikzpicture}
  \caption{Example sketch of a decision tree. The tree decides whether an input data point is of class 1 or class 2. The available features are \texttt{feature\_1}, \texttt{feature\_2} and \texttt{feature\_3}. This decision tree has a depth of 2 and solves the task of a binary separation at each leaf (green boxes). The decision thresholds are written next to the respective connecting lines.}
  \label{fig:tree}
\end{figure}
%
To determine the best decision threshold for each leaf, a performance measure
for the information gain is necessary. For a classification task the required
metric would be the accuracy. The thresholds are thus optimized to get the best
accuracy at the respective leaf. This way the decision tree is build from the
top leaf downwards until a perfect classification is achieved or until a set
maximum depth is reached.

\subsection{Random forests}
%
